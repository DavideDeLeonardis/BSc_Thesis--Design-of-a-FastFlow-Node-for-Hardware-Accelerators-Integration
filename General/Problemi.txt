1. Il "Deadlock Fantasma" (La Pipeline che Non Partiva)
Questo è stato in assoluto il problema più difficile e subdolo.
Il Sintomo: Il programma compilava perfettamente, partiva, stampava Configuration: ... e poi terminava immediatamente (o si bloccava). L'output finale era sempre Verification: Tasks processed: 0 / N (FAILURE).
Perché era così Difficile: Non c'era un crash o un errore. Il programma "funzionava", ma non faceva nulla. Questo ci ha fatto dubitare di tutto: della logica della promise/future (che invece era corretta), del Collector (che abbiamo rimosso e poi rimesso), e della gestione della memoria (new Task).
La Causa Finale: Era una race condition (condizione di gara) fondamentale nel nostro design. La pipeline di FastFlow era così veloce che l'Emitter riusciva a inviare tutti i task e il segnale di EOS prima che i nostri thread interni (producerLoop e consumerLoop) avessero il tempo materiale di essere schedulati dal sistema operativo e iniziare a lavorare. Il framework, vedendo che non c'era più nulla da fare, avviava la procedura di spegnimento e run_and_wait_end() terminava, prima che un singolo task fosse stato calcolato.
Il Tempo Perso: Ci abbiamo messo così tanto tempo perché abbiamo dovuto provare molte diverse strategie di terminazione (svc_end, distruttore) e ognuna creava un deadlock diverso, fino a spingerci a ri-architettare completamente la logica interna verso il modello a 3 thread che stiamo implementando ora.

2. Il Conflitto Infernale dei Compilatori sulla VM
Questo è stato il secondo problema più frustrante, perché era un problema di ambiente esterno, non di codice.
Il Sintomo: Il programma crashava con Segmentation fault oppure non partiva affatto con l'errore GLIBCXX_3.4.32 not found.
Perché era così Difficile: La causa era un conflitto tra due diversi compilatori C++ installati sulla VM: il compilatore di sistema (un g++ stabile) e un compilatore sperimentale (g++-15) installato da Vitis che "inquinava" l'ambiente. Ogni soluzione che provavamo falliva per un motivo diverso:
Il linking statico (-static-libstdc++) risolveva il GLIBCXX ma creava un crash.
LD_LIBRARY_PATH non funzionava perché l'eseguibile era linkato dinamicamente alla libreria sbagliata.
I comandi cmake -D CMAKE_CXX_COMPILER=... venivano ignorati perché gli script di Vitis avevano una priorità più alta.
Il Tempo Perso: Abbiamo dovuto indagare a fondo l'ambiente della VM, modificando il .bashrc, cercando manualmente i compilatori (ls /usr/bin/g++*) e infine trovando la soluzione definitiva: forzare l'uso di g++-11 direttamente nel CMakeLists.txt per "blindare" la compilazione.
Gli altri problemi, come l'errore Operation not permitted (risolto con sudo) o l'errore CL_INVALID_WORK_GROUP_SIZE (risolto usando global_work_size = 1), erano più semplici perché erano errori "onesti" che ci dicevano esattamente cosa non andava. I primi due, invece, erano problemi di concorrenza e di ambiente, i più difficili da diagnosticare in assoluto.

A. Il Problema: Il Collo di Bottiglia Sincrono (Il Peccato Originale)
Cos'era: La tua architettura di partenza era sincrona. Il ff_node_acc_t originale, sebbene avesse due thread, alla fine aveva un producerLoop che chiamava accelerator_->execute(...). Quella singola chiamata faceva tutto: trasferiva i dati, eseguiva il kernel e recuperava i risultati.
https://github.com/DavideDeLeonardis/TESI--Progettazione-di-un-nodo-FastFlow-per-integrazione-di-acceleratori/blob/2be8362006214ae3eb78e2ed7eec1fe48391a4d3/src/GpuAccelerator.cpp
Perché era un Problema: L'intero nodo restava bloccato in attesa che un singolo task completasse l'intero ciclo. Non c'era alcuna sovrapposizione. La GPU/FPGA passava la maggior parte del suo tempo inattiva, aspettando che arrivassero nuovi dati. [Immagine di un operaio in attesa in una catena di montaggio]
La Soluzione (Il Primo Cambio di Architettura): Questo è stato il motivo principale della nostra profonda ristrutturazione. Abbiamo dovuto:
Smantellare la funzione monolitica execute().
Creare un'interfaccia asincrona (send_data_async, execute_kernel_async, get_results_blocking).
Riprogettare completamente ff_node_acc_t trasformandolo da un semplice "consumer" a una micro-pipeline interna a 3 stadi (uploader, launcher, downloader) per gestire questa nuova interfaccia.

B. Il Problema: L'Esecuzione "Falsamente Parallela" della CPU
Cos'era: Dopo aver costruito la nostra bellissima pipeline asincrona a 3 stadi, abbiamo notato che i log della CPU erano perfettamente ordinati (START...END, START...END).
Perché era un Problema: Ci ha rivelato una verità fondamentale: la nostra pipeline era progettata per l'offloading (delegare il lavoro a un hardware separato). La CPU non può fare offload a se stessa. Le sue funzioni _async erano vuote e tutto il lavoro si accumulava nell'ultimo stadio (downloaderLoop), rendendo di fatto l'intera pipeline sequenziale.
La Soluzione (Il Secondo Cambio di Architettura): Abbiamo capito che per la CPU serviva un pattern di parallelismo diverso. Questo ci ha portato a progettare una biforcazione nel main:
Per GPU/FPGA: Manteniamo la nostra pipeline di offloading a 3 stadi.
Per CPU: Usiamo un pattern "Farm" (ff_farm) con una squadra di CpuWorker che processano i task in vero parallelo.

2. I Bug Tecnici più Difficili da Risolvere

A. Il Problema: Il Deadlock (Race Condition sulla Terminazione)
Cos'era: Dopo aver implementato la pipeline a 3 stadi, il programma con la CPU eseguiva tutti i task ma non terminava mai. Si bloccava.
Perché era Difficile: Questo era un bug di concorrenza (race condition). I nostri 3 thread interni aspettavano il segnale SENTINEL per terminare. Ma in alcuni casi, il runtime di FastFlow chiamava svc_end() (che faceva il join sui thread) prima che svc() avesse ricevuto FF_EOS per inviare il SENTINEL. I thread non avrebbero mai ricevuto il segnale, e il join non sarebbe mai terminato.
La Soluzione: Aggiungere una "sentinella di sicurezza" all'inizio di svc_end(), per garantire che, in qualsiasi scenario di terminazione, la nostra pipeline interna ricevesse il segnale di spegnimento.

B. Il Problema: Gli Errori OpenCL Criptici
Cos'era: L'esecuzione su GPU falliva con errori numerici come -36 o -38.
Perché era Difficile:
Errore -36 (CL_INVALID_COMMAND_QUEUE): Questo è stato un problema di compatibilità ambientale. Avevamo aggiunto il flag CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE per ottimizzare, ma l'implementazione OpenCL di macOS non lo supporta. L'errore non diceva "flag non supportato", ma solo "coda invalida", il che ha reso il debug difficile.
Errore -38 (CL_INVALID_MEM_OBJECT): Questo era un errore a cascata. Inserendo N=0, la nostra logica di allocazione falliva (non si può creare un buffer di dimensione 0), ma il codice non controllava questo fallimento e procedeva, cercando di usare un puntatore a un buffer invalido. Questo portava all'errore -38 e infine a un Segmentation Fault.
La Soluzione:
Rimuovere il flag OUT_OF_ORDER per massimizzare la compatibilità.
Aggiungere un controllo di validazione dell'input nel main per intercettare N=0 prima ancora di avviare la pipeline.

L'Argomentazione: Il Problema del "Nastro Trasportatore Rotto"
Il problema che ci ha costretto a cambiare l'architettura è stata una contraddizione logica che emergeva dai nostri risultati:
Il Calcolo AVVENIVA: La prova inconfutabile era che il tempo computed era maggiore di zero. Poiché questa variabile viene incrementata solo dal producerLoop (dentro accNode), questo significava che il producerLoop stava funzionando, i Task venivano processati, e i Result venivano creati e inviati al consumerLoop.
I Risultati NON ARRIVAVANO: La verifica Tasks processed: 0 / N (FAILURE) ci diceva, con altrettanta certezza, che il metodo svc del Collector non veniva mai chiamato.
La Diagnosi
Mettendo insieme le due prove, la conclusione era una sola: i nostri thread interni (producerLoop e consumerLoop) funzionavano perfettamente, ma i risultati, una volta inviati dal consumerLoop alla pipeline tramite ff_send_out(), si "perdevano" e non raggiungevano mai il Collector.
Era come avere una catena di montaggio in cui la stazione di assemblaggio (accNode) funzionava e metteva i prodotti finiti sul nastro trasportatore finale, ma l'ispettore qualità in fondo alla linea (Collector) non riceveva nulla.
Il problema era il "nastro trasportatore" tra i due nodi.
La Causa Tecnica (La Race Condition)
Questa "rottura" era causata da una race condition (condizione di gara) molto complessa e difficile da risolvere:
Il nostro accNode è un nodo "attivo" che invia dati dall'interno di un thread (consumerLoop).
La pipeline (ff_Pipe) riceveva questi dati, ma riceveva anche il segnale di fine stream (EOS) quasi contemporaneamente.
Il framework non riusciva a gestire in modo affidabile questa situazione: avviava la terminazione prima di aver consegnato tutti i pacchi in sospeso al Collector.
La Soluzione Architetturale (Da 3 a 2 Stadi)
Invece di tentare di "riparare" un comportamento inaffidabile del framework (il nastro trasportatore rotto), abbiamo scelto una soluzione architetturale più robusta: eliminare del tutto quel pezzo della catena di montaggio.
Abbiamo "promosso" il consumerLoop da semplice operaio a ispettore qualità finale. Le sue responsabilità sono aumentate:
Riceve i Result dal producerLoop.
Li conta (il lavoro che prima faceva il Collector).
Pulisce la memoria (altro lavoro del Collector).
Comunica il conteggio finale al main (tramite la promise).
Trasformando la pipeline da Emitter -> accNode -> Collector a Emitter -> accNode, abbiamo eliminato il "nastro trasportatore" problematico e risolto il FAILURE alla radice, ottenendo un sistema stabile e funzionante che rispetta comunque tutti i requisiti della tesi (code, thread interni e offloading).

1. La Sfida Architetturale: L'Inefficienza dell'Attesa Attiva (yield())
Il primo e più grande problema che ci ha spinto a cambiare architettura era nel design originale del suo ff_node_acc_t.
Problema: La sua implementazione iniziale, sebbene intelligente, usava code personalizzate e un ciclo while(...) std::this_thread::yield(); per l'attesa. Questa è chiamata attesa attiva (busy-waiting).
Perché è un problema: Il thread (producerLoop o consumerLoop) non "dormiva" mai. Continuava a consumare il 100% di un core della CPU solo per chiedere "È pronto? È pronto? È pronto?". Questo spreca un'enorme quantità di risorse e rende l'intero sistema inefficiente.
Soluzione (Il Cambio di Architettura): Abbiamo eliminato le sue code personalizzate e yield() e le abbiamo sostituite con una BlockingQueue (basata su std::mutex e std::condition_variable). Questo è stato il refactoring più importante:
Ora, quando un thread (consumerLoop) aspetta un task, il sistema operativo lo mette a dormire (wait()).
Il thread consuma 0% di CPU durante l'attesa.
Viene "svegliato" magicamente (notify_one()) solo quando l'altro thread (producerLoop) ha un nuovo lavoro per lui.
Risultato: Siamo passati da un design inefficiente che sprecava CPU a un'architettura efficiente, moderna e standard per il multi-threading.

2. Il Bug più Difficile da Risolvere: Il Deadlock di Terminazione
Questo è stato il bug più frustrante e sottile che abbiamo affrontato.
Problema: Dopo aver implementato la BlockingQueue, il programma eseguiva tutti i task, stampava [Main] FF Pipeline execution finished., ma poi si bloccava (hang/freeze), senza mai stampare le statistiche finali.
Diagnosi: Il programma era bloccato su final_count = count_future.get(). Questo significava che il consumerLoop (l'unico che poteva impostare la promise) non stava terminando correttamente. A sua volta, questo accadeva perché il thread principale di FastFlow e i nostri thread interni si stavano aspettando a vicenda in un circolo vizioso (un deadlock).
Errore Logico (le nostre varie soluzioni errate): Abbiamo provato a mettere i join() nel distruttore (causava un crash) o a inviare un SENTINEL da svc_end (non funzionava).
Soluzione Definitiva (La Logica Corretta): Abbiamo stabilito il ciclo di vita corretto per la terminazione:
Il metodo svc() ha la responsabilità di rilevare la fine (FF_EOS).
Quando rileva FF_EOS, svc() avvia la terminazione interna (inviando SENTINEL alla inQ_) E notifica FastFlow che ha finito (restituendo FF_EOS).
Il metodo svc_end(), che FastFlow chiama solo dopo che svc ha restituito FF_EOS, ha la responsabilità di attendere la fine dei thread (join()).
Questo ha risolto il deadlock e ha reso la terminazione robusta.

3. La Sfida di Portabilità: macOS vs. Linux (CMake e Metal)
Questa è stata una sfida puramente tecnica ma molto complessa.
Problema: Volevamo usare OpenMP e FPGA su Linux, e OpenCL e Metal su macOS. Il nostro file CMakeLists.txt non era in grado di gestire questa complessità.
Errori Incontrati:
g++: error: language objective-c++ not recognized (Linux non sa cos'è Metal).
ld: library 'stdc++fs' not found (macOS non ha questa libreria).
Soluzione: Abbiamo trasformato il CMakeLists.txt in un file "intelligente" che usa le variabili di CMake (come if(APPLE)) per compilare e linkare condizionalmente i file sorgente e le librerie corrette per ogni sistema operativo.
Bug di Interoperabilità (Metal): L'integrazione di Metal (Objective-C) con il C++ ci ha dato errori (no type or protocol named 'MTLDevice', __bridge_retained casts have no effect).
Soluzione: Abbiamo usato due tecniche:
Header Puli ("Pimpl Idiom"): Abbiamo usato void* nel file .hpp (puro C++) per nascondere i tipi di Metal.
Bridging Cast: Abbiamo usato __bridge_retained e __bridge_transfer nel file .mm (Objective-C++) per gestire correttamente la proprietà della memoria tra il C++ e il sistema ARC di Apple.

4. L'Errore Logico più Insidioso: La Compilazione FPGA di 15 Ore
Questo è stato l'errore logico che ha richiesto più tempo "reale".
Problema: Il kernel heavy_compute_kernel per FPGA compilava, ma il processo di sintesi rimaneva bloccato per 15+ ore su un singolo job.
Errore Logico: Avevamo applicato la direttiva #pragma HLS PIPELINE al loop esterno (for i...).
Perché è un Errore: Stavamo chiedendo a Vitis HLS di creare un hardware in grado di iniziare un'intera iterazione di N elementi ogni singolo ciclo di clock. Per fare questo, ha tentato di srotolare (unroll) completamente il ciclo interno (for j...), creando un circuito mostruoso di 200 unità sin/cos in parallelo che l'FPGA non poteva fisicamente gestire.
Soluzione: Abbiamo spostato il #pragma HLS PIPELINE al loop interno. Questo ha cambiato l'istruzione in: "Crea una pipeline efficiente per il ciclo interno di 200 iterazioni". La compilazione è passata da 15+ ore a 3 minuti e 39 secondi.

1. Il Difetto Logico della CPU: Il "Falso Parallelismo"
Questo è stato il primo e più importante errore concettuale che abbiamo corretto, fondamentale per la validità della sua tesi.
Problema: Inizialmente, lei aveva una classe CpuAccelerator che simulava di essere un acceleratore. Per eseguire il calcolo, implementava un semplice ciclo for sequenziale all'interno della funzione get_results_blocking.
Perché era un Errore Logico: Confrontare un'architettura hardware altamente parallela (GPU/FPGA) con un'esecuzione su un singolo core della CPU non è un confronto equo. La traccia della tesi richiedeva esplicitamente il "parallelismo shared memory/thread sulla CPU multicore".
Soluzione (Cambio di Architettura): Abbiamo eliminato la classe CpuAccelerator. L'abbiamo sostituita con un percorso di esecuzione completamente separato nel main che chiama le funzioni executeCpu_FF_Tasks e executeCpu_OMP_Tasks. Queste funzioni usano ff::ParallelFor e #pragma omp parallel for, implementando il vero parallelismo su CPU e fornendo una baseline di confronto robusta e corretta.


Durante lo sviluppo, è emersa un'incompatibilità di versione tra la piattaforma Xilinx (2022.x) e gli strumenti Vitis utilizzati (2023.x), che ha impedito il completamento della fase di emulazione hardware (hw_emu). 
La verifica funzionale è stata completata con successo tramite emulazione software (sw_emu) e la validazione finale delle performance è stata eseguita direttamente sull'hardware fisico.