!! Aggiungi citazioni
!! Aggiungi captions a immagini e riferimenti dal testo

Font: Cambria
Codice: Courier New
Capitoli: 18pt
Sezioni: 16pt
Sottosezione: 14pt
Testo: 12pt
4 righe fra sottosezioni diverse

TITOLO:
   Progettazione di un nodo FastFlow per integrazione di acceleratori

Capitolo 1: Introduzione
   1.1 Contesto: L'Era del Calcolo Eterogeneo
   Il rallentamento della Legge di Moore e la necessità di performance.
   L'ascesa del calcolo eterogeneo (CPU, GPU, FPGA) come soluzione.
   1.2 Il Problema: Complessità di Integrazione
   La potenza degli acceleratori (GPU per il parallelismo di massa, FPGA per la specializzazione) vs. la difficoltà di programmarli e integrarli in applicazioni C++ esistenti.
   1.3 La Soluzione Proposta: FastFlow come Orchestratore
   Introduzione a FastFlow come framework di alto livello per il parallelismo C++.
   Spiegare come FastFlow possa "nascondere" la complessità dell'offloading hardware.
   1.4 Obiettivi e Contributi della Tesi
   Obiettivo: Si propone di realizzare un semplice prototipo per l'integrazione di kernel programmati su FPGA Alveo mediante Vitis (sostanzialmente kernel scritti in C/C++) nell'ambiente di programmazione FastFlow. Verranno utilizzati kernel esistenti.
   L'obiettivo della tesi sarà quello di interfacciare i kernel mediante codice OpenCL sfruttando tutte le possibilità offerte da tale ambiente. L'integrazione in FastFlow dovrà permettere di utilizzare sia il classico parallelismo shared memory/thread sulla CPU multicore che l'offloading di computazioni pesanti sull'acceleratore FPGA.
   Progettare, implementare e valutare un nodo FastFlow per l'integrazione efficiente di kernel per acceleratori hardware (FPGA, GPU OpenCL, GPU Metal) all'interno di una pipeline FastFlow.
   Contributi: Il design di un'architettura software disaccoppiata; l'analisi comparativa delle performance (Latenza, Throughput, Overhead) tra diverse API e carichi di lavoro (memory-bound vs. compute-bound).
   Intro: Lezione sulla Legge di Amdahl e l'Ammortamento
   Questi risultati sono una perfetta dimostrazione pratica della Legge di Amdahl. La parte "seriale" del nostro programma (l'inizializzazione di OpenCL) è così lenta che, anche se acceleriamo la parte "parallela" (il calcolo), il guadagno totale è negativo per un singolo task.
   Questo dimostra in modo conclusivo che l'offloading su acceleratori ha senso solo in scenari ad alto throughput, dove il costo di setup viene "spalmato" su un gran numero di operazioni.


Capitolo 2: Fondamenti e Tecnologie Abilitanti
      2.1 Architetture a Confronto: CPU, GPU, FPGA
      Architettura CPU (Multi-core, cache, shared memory).
      Architettura GPU (SIMT, migliaia di core, parallelismo dei dati).
      Architettura FPGA (Logica riconfigurabile, parallelismo a pipeline, Sintesi ad Alto Livello (HLS) e ruolo di Vitis).
      2.2 Framework di Parallelismo Software su CPU
      FastFlow: Spiegare i pattern ff_Pipe (pipeline) e ff::ParallelFor (parallelismo dati).
      OpenMP: Spiegare il modello a direttive (#pragma omp parallel for) come standard industriale.
      2.3 API per l'Offloading Hardware
      OpenCL: Spiegare il suo ruolo di standard multipiattaforma (Contesto, Coda, Kernel .cl, Eventi).
      Apple Metal: Spiegare il suo ruolo di API nativa ad alte prestazioni per macOS (Device, Coda, Kernel .metal, Memoria Unificata).


Capitolo 3: Progettazione dell'Architettura Software - Progetto logico
      (Il "come" e il "perché" del suo design)
      3.1 Visione d'Insieme: Un Dispatcher Flessibile
      Descrivere l'architettura del main come "dispatcher" che seleziona il percorso di esecuzione (CPU vs. Acceleratore) in base agli input.
      3.2 L'Interfaccia IAccelerator: Il Contratto di Astrazione
      Spiegare l'importanza di un'interfaccia pura per disaccoppiare la logica della pipeline dall'hardware specifico.
      3.3 Architettura 1: Esecuzione Parallela su CPU - Desing pattern usato
      Spiegare il design dei "Runner" (executeCpuFFTasks, executeCpuOMPTasks) e perché sono sequenziali a livello di task ma paralleli a livello di dati.
      3.4 Architettura 2: La Pipeline di Offloading (ff_node_acc_t)
      Il Problema del Design: Come gestire l'offloading asincrono e "stateful" (con stato) all'interno di un nodo FastFlow?
      La Soluzione (Il Cuore della Tesi): L'incapsulamento di una pipeline manuale a 2 stadi (producer/consumer).
      Giustificazione: Spiegare perché questa architettura è superiore a una ff_Pipe "piatta" (gestione centralizzata dello stato, del BufferManager e della sincronizzazione).
      Sincronizzazione Efficiente: Spiegare la scelta della BlockingQueue per eliminare l'attesa attiva (yield) e consumare 0% di CPU.
      3.5 Pattern di Composizione: Il BufferManager
      Spiegare perché la logica del pool di buffer è stata estratta in una classe separata (Composizione) invece di usare una classe base complessa (Ereditarietà), per eliminare la duplicazione di codice tra Gpu e Fpga_Accelerator.


Capitolo 4: Dettagli Implementativi
      (Il "codice" che realizza il design)
      ff_node_acc_t
      4.2 Implementazione degli Acceleratori Concreti
      Gpu_OpenCL_Accelerator: Compilazione da sorgente (clCreateProgramWithSource).
      Fpga_Accelerator: Caricamento di binari (clCreateProgramWithBinary).
      Gpu_Metal_Accelerator: L'uso di Objective-C++ (.mm), la gestione della memoria unificata (memcpy), la compilazione da sorgente (newLibraryWithSource) e i "bridging cast" per nascondere i tipi (void*).
      4.3 Implementazione della Portabilità (CMake)
      Spiegare come CMakeLists.txt usa le direttive condizionali (if(APPLE)) per compilare e linkare file diversi (es. Fpga_Accelerator.cpp solo su Linux, Gpu_Metal_Accelerator.mm solo su macOS).


Capitolo 5: Analisi Sperimentale e Risultati
   5.1 Setup Sperimentale (Hardware e Software)
   Descrivere in dettaglio le macchine (Mac M2 Pro, Host linux con ubunti con cpu Intel e Scheda FPGA Alveo U50 - comando per vedere dati delle schede) e le versioni dei software (Vitis, FastFlow, Clang, GCC).
   5.2 Metodologia di Benchmark e Metriche
   Spiegare i test (N=10k, 1M, 7.4M; NUM_TASKS=100).
   Definizione delle Metriche: Spiegare la legenda (Avg In-Node Time, Avg Service Time, Throughput, Compute, Overhead).
   5.3 Presentazione dei Risultati
   Includere le tabelle complete dei dati.
   Grafici Chiave: Throughput vs. N; Latenza vs. Periodo; Breakdown del tempo (Compute vs. Overhead) per i diversi kernel.
   5.4 Discussione e Analisi dei Risultati
   Validazione della Pipeline: Usare i dati (Latenza >> Periodo) per dimostrare che la sovrapposizione funziona.
   Analisi del Collo di Bottiglia: Dimostrare come il sistema sia limitato dall'overhead (memory/call-bound) con i kernel leggeri, e come diventi limitato dal calcolo (compute-bound) con heavy_compute_kernel.
   L'Overhead di Accodamento: Spiegare che l'alto Avg Overhead Time nei casi compute-bound è un sintomo di successo (tempo di attesa in coda) che prova la saturazione del collo di bottiglia.
   Confronti Piattaforma:
   CPU: FastFlow vs. OpenMP (perché OpenMP è risultato più veloce?).
   GPU: OpenCL vs. Metal (quantificare il costo dell'astrazione vs. l'API nativa).
   FPGA: Analisi delle performance HLS (perché è più lento? Overhead VM/PCIe, design non ottimizzato per la forza bruta).
      Cambio funzioni da enqueueBuffer in from explicit copy to map and migrate in FPGA Accelerator e GPU Accelerator
      Ho proivato a fare from explicit copy to map and migrate (vedi branch apposita) e si aveva effettivamente un incremento minimo nelle prestazioni ma si complicava ulteriormente il codice
      Es. con FPGA 
         ------------------------------------------------------------------
         PERFORMANCE METRICS on FPGA
            (N=7449999, Tasks=100, Kernel=krnl_heavy_compute)
         ------------------------------------------------------------------
         Avg Service Time: 2494.37 ms/task
            (Tempo medio tra il completamento di due task consecutivi)

         Avg In_Node Time: 132413 ms/task
            (Tempo medio per un task dall'ingresso all'uscita del nodo)

         Avg Pure Compute Time: 2545.6 ms/task
            (Tempo medio di un singolo calcolo sull'acceleratore, senza overhead)

         Avg Overhead Time: 129867 ms/task
            (Costo medio di gestione: trasferimento dati, uso delle code, etc.)

         Throughput: 0.392308 tasks/sec
            (Task totali processati al secondo)

         Total Time Elapsed: 254.902 s
         ------------------------------------------------------------------


Capitolo 6: Conclusioni
   6.1 Sintesi del Lavoro e Raggiungimento degli Obiettivi
   6.2 Analisi critica: Limitazioni e Problemi Riscontrati
      (limite N su FPGA, 
      2 input e 1 output
      nome kernel uguale a nome kernel file, 
      incompatibilità di versione Vitis, 
      PROBLEMI,
      ecc.).
   6.4 Considerazioni Personali
      (Cosa ha imparato dal progetto, corsi universitari più utili, difficoltà e soddisfazioni).
      Questa analisi evidenzia un punto fondamentale: le CPU e le GPU moderne sono macchine potentissime e altamente ottimizzate per questo tipo di accesso lineare alla memoria, dotate di cache sofisticate e bus di memoria molto ampi.
      La vera forza di un'FPGA non è eseguire task semplici, ma implementare pipeline di calcolo complesse e customizzate che non esistono su una CPU/GPU, ma dove i dati fluiscono attraverso una catena di montaggio hardware su misura.
      Per un task così semplice come una somma vettoriale, l'overhead di comunicazione con la scheda e la sua architettura di memoria generica non riescono a competere. Usare un'FPGA per una somma vettoriale è come usare un bisturi chirurgico specializzato per aprire una scatola di cartone: funziona, ma un semplice taglierino (la CPU/GPU) è più efficiente per quel compito specifico.

      // Perché Non è Stata Usata una Pipeline FastFlow Interna?
         L'architettura a 3 stadi con questo nodo asincrono è instabile in FastFlow => uso 2 stadi (Emitter, accNode)
         Sebbene in teoria sembri più elegante annidare una ff_Pipe all'interno del ff_node_acc_t, questa soluzione è inferiore e più complessa a causa della gestione dello stato condiviso.
         L'operazione di offloading richiede che lo stato di un singolo task (IAccelerator e le sue risorse come l'indice del buffer usato e gli eventi OpenCL per la sincronizzazione) sia mantenuto e passato coerentemente tra i nodi di Upload, Launch e Download.
         Se avessimo usato tre nodi FastFlow separati (UploaderNode, LauncherNode, DownloaderNode), avremmo dovuto implementare complessi meccanismi di sincronizzazione esterni (ad esempio, mappe globali protette da mutex) per condividere l'oggetto accelerator e per associare lo stato corretto a ogni task che fluisce attraverso nodi indipendenti. 
         Questo avrebbe reso il codice più difficile da leggere, più prono a errori (race condition) e avrebbe reintrodotto la complessità che il framework dovrebbe nascondere.
         L'architettura attuale, con una pipeline manuale incapsulata in ff_node_acc_t, è superiore perché:
            Incapsula lo Stato: Il ff_node_acc_t "possiede" l'acceleratore e tutte le sue risorse (come il pool di buffer). Tutta la logica e lo stato sono confinati in un unico componente.
            Semplifica la Logica: La comunicazione tra i thread interni è banale, poiché condividono lo stato dello stesso oggetto.

      // Se volessi sostituire la somma vettoriale con un altro kernel, per esempio una moltiplicazione tra matrici, non dovresti toccare l'infrastruttura principale del progetto (ff_node_acc_t, la logica della pipeline, etc.).
         Le modifiche sarebbero mirate e contenute solo nelle parti specifiche del task.
            File da Modificare
               1. I Nuovi File dei Kernel (L'Algoritmo)
                  Prima di tutto, dovresti scrivere il nuovo algoritmo. Non modificheresti i file esistenti, ma ne creeresti di nuovi.
                  Per la GPU: Creeresti un nuovo file, ad esempio matrix_mul.cl, contenente il kernel OpenCL C per la moltiplicazione di matrici.
                  Per l'FPGA: Creeresti un nuovo file C++, ad esempio kernel/krnl_matmul.cpp, con la logica per la sintesi HLS della moltiplicazione di matrici.

               2. Il File dei Tipi Dati (include/types.hpp)
                  La struttura Task attuale è specifica per la somma vettoriale (tre vettori, una dimensione). Una moltiplicazione di matrici ha bisogno di dati diversi.
                  Azione: Aggiungeresti una nuova struct per descrivere il nuovo lavoro, per esempio:
                  // Esempio per una moltiplicazione di matrici C = A x B
                  struct MatrixMulTask {
                     float *matrix_a, *matrix_b, *matrix_c;
                     int width_a, height_a, width_b;
                  };
                  Anche la struct Result potrebbe dover cambiare se il risultato fosse più complesso.

               3. Le Classi "Adapter" (Cpu/Gpu/Fpga_Accelerator.cpp)
                  Qui è dove "collegheresti" il nuovo task e il nuovo kernel. Ogni adapter andrebbe modificato.
                  CpuAccelerator.cpp:
                  Nel metodo execute, la chiamata a std::transform verrebbe sostituita con due cicli for annidati per eseguire la moltiplicazione di matrici sulla CPU.
                  GpuAccelerator.cpp e Fpga_Accelerator.cpp:
                  Nel metodo initialize, cambieresti il nome del file del kernel da caricare (es. matrix_mul.cl o il nuovo .xclbin) e il nome del kernel da estrarre (es. "matrix_mul").
                  Nel metodo execute, faresti le modifiche più sostanziose:
                  Faresti il cast del puntatore generic_task al nuovo tipo MatrixMulTask*.
                  Le chiamate clCreateBuffer creerebbero buffer con le dimensioni corrette per le matrici.
                  Le chiamate clSetKernelArg passerebbero i puntatori alle matrici e le loro dimensioni, in accordo con la firma del nuovo kernel.

               4. La Classe Emitter (in main.cpp)
                  L'Emitter è responsabile di creare i dati di test.
                  Azione: Modificheresti il costruttore dell'Emitter per inizializzare due matrici di input (a e b) con valori sensati. Anche la chiamata per creare il task diventerebbe return new MatrixMulTask{...};.

            File che NON Andrebbero Modificati (La Prova della Buona Architettura)
               IAccelerator.hpp: L'interfaccia, il "contratto", non cambia. Definisce ancora le azioni generiche initialize ed execute.
               ff_node_acc_t.hpp e ff_node_acc_t.cpp: Il tuo "motore" asincrono a thread e code rimarrebbe identico. Non gli interessa quale calcolo venga eseguito; il suo compito è solo orchestrare il flusso di void* tra le code e delegare il lavoro all'acceleratore.
               La funzione main() in main.cpp: La logica di creazione della pipeline, la misurazione dei tempi e il meccanismo di verifica con promise/future rimarrebbero esattamente gli stessi. L'unica riga da cambiare sarebbe quella che crea l'adapter specifico (es. accelerator = std::make_unique<MatrixMulGpuAccelerator>();).


Bibliografia

Codice

Ringraziamenti