!! Migliora diagramma delle classi
!! Aggiungi citazioni
!! Aggiungi captions a immagini

Font: Cambria
Codice: Courier New
Capitoli: 18pt
Sezioni: 16pt
Sottosezione: 14pt
Testo: 12pt
4 righe fra sottosezioni diverse

TITOLO:
   Progettazione di un nodo FastFlow per integrazione di acceleratori

Capitolo 1: Introduzione
1.1 Contesto: L'Era del Calcolo Eterogeneo
Il rallentamento della Legge di Moore e la necessità di performance.
L'ascesa del calcolo eterogeneo (CPU, GPU, FPGA) come soluzione.
1.2 Il Problema: Complessità di Integrazione
La potenza degli acceleratori (GPU per il parallelismo di massa, FPGA per la specializzazione) vs. la difficoltà di programmarli e integrarli in applicazioni C++ esistenti.
1.3 La Soluzione Proposta: FastFlow come Orchestratore
Introduzione a FastFlow come framework di alto livello per il parallelismo C++.
Spiegare come FastFlow possa "nascondere" la complessità dell'offloading hardware.
1.4 Obiettivi e Contributi della Tesi
Obiettivo: Si propone di realizzare un semplice prototipo per l'integrazione di kernel programmati su FPGA Alveo mediante Vitis (sostanzialmente kernel scritti in C/C++) nell'ambiente di programmazione FastFlow. Verranno utilizzati kernel esistenti.
L'obiettivo della tesi sarà quello di interfacciare i kernel mediante codice OpenCL sfruttando tutte le possibilità offerte da tale ambiente. L'integrazione in FastFlow dovrà permettere di utilizzare sia il classico parallelismo shared memory/thread sulla CPU multicore che l'offloading di computazioni pesanti sull'acceleratore FPGA.
Progettare, implementare e valutare un nodo FastFlow per l'integrazione efficiente di kernel per acceleratori hardware (FPGA, GPU OpenCL, GPU Metal) all'interno di una pipeline FastFlow.
Contributi: Il design di un'architettura software disaccoppiata; l'analisi comparativa delle performance (Latenza, Throughput, Overhead) tra diverse API e carichi di lavoro (memory-bound vs. compute-bound).

Capitolo 2: Fondamenti e Tecnologie Abilitanti
2.1 Architetture a Confronto: CPU, GPU, FPGA
Architettura CPU (Multi-core, cache, shared memory).
Architettura GPU (SIMT, migliaia di core, parallelismo dei dati).
Architettura FPGA (Logica riconfigurabile, parallelismo a pipeline, Sintesi ad Alto Livello (HLS) e ruolo di Vitis).
2.2 Framework di Parallelismo Software su CPU
FastFlow: Spiegare i pattern ff_Pipe (pipeline) e ff::ParallelFor (parallelismo dati).
OpenMP: Spiegare il modello a direttive (#pragma omp parallel for) come standard industriale.
2.3 API per l'Offloading Hardware
OpenCL: Spiegare il suo ruolo di standard multipiattaforma (Contesto, Coda, Kernel .cl, Eventi).
Apple Metal: Spiegare il suo ruolo di API nativa ad alte prestazioni per macOS (Device, Coda, Kernel .metal, Memoria Unificata).

Capitolo 3: Progettazione dell'Architettura Software - Progetto logico
(Il "come" e il "perché" del suo design)
3.1 Visione d'Insieme: Un Dispatcher Flessibile
Descrivere l'architettura del main come "dispatcher" che seleziona il percorso di esecuzione (CPU vs. Acceleratore) in base agli input.
3.2 L'Interfaccia IAccelerator: Il Contratto di Astrazione
Spiegare l'importanza di un'interfaccia pura per disaccoppiare la logica della pipeline dall'hardware specifico.
3.3 Architettura 1: Esecuzione Parallela su CPU - Desing pattern usato
Spiegare il design dei "Runner" (executeCpuFFTasks, executeCpuOMPTasks) e perché sono sequenziali a livello di task ma paralleli a livello di dati.
3.4 Architettura 2: La Pipeline di Offloading (ff_node_acc_t)
Il Problema del Design: Come gestire l'offloading asincrono e "stateful" (con stato) all'interno di un nodo FastFlow?
La Soluzione (Il Cuore della Tesi): L'incapsulamento di una pipeline manuale a 2 stadi (producer/consumer).
Giustificazione: Spiegare perché questa architettura è superiore a una ff_Pipe "piatta" (gestione centralizzata dello stato, del BufferManager e della sincronizzazione).
Sincronizzazione Efficiente: Spiegare la scelta della BlockingQueue per eliminare l'attesa attiva (yield) e consumare 0% di CPU.
3.5 Pattern di Composizione: Il BufferManager
Spiegare perché la logica del pool di buffer è stata estratta in una classe separata (Composizione) invece di usare una classe base complessa (Ereditarietà), per eliminare la duplicazione di codice tra Gpu e Fpga_Accelerator.

Capitolo 4: Dettagli Implementativi
(Il "codice" che realizza il design)
ff_node_acc_t
4.2 Implementazione degli Acceleratori Concreti
Gpu_OpenCL_Accelerator: Compilazione da sorgente (clCreateProgramWithSource).
Fpga_Accelerator: Caricamento di binari (clCreateProgramWithBinary).
Gpu_Metal_Accelerator: L'uso di Objective-C++ (.mm), la gestione della memoria unificata (memcpy), la compilazione da sorgente (newLibraryWithSource) e i "bridging cast" per nascondere i tipi (void*).
4.3 Implementazione della Portabilità (CMake)
Spiegare come CMakeLists.txt usa le direttive condizionali (if(APPLE)) per compilare e linkare file diversi (es. Fpga_Accelerator.cpp solo su Linux, Gpu_Metal_Accelerator.mm solo su macOS).

Capitolo 5: Analisi Sperimentale e Risultati
(Il capitolo più importante: i dati)
5.1 Setup Sperimentale (Hardware e Software)
Descrivere in dettaglio le macchine (Mac M2 Pro, Host linux con ubunti con cpu Intel e Scheda FPGA Alveo U50 - comando per vedere dati delle schede) e le versioni dei software (Vitis, FastFlow, Clang, GCC).
5.2 Metodologia di Benchmark e Metriche
Spiegare i test (N=10k, 1M, 7.4M; NUM_TASKS=100).
Definizione delle Metriche: Spiegare la legenda (Avg In-Node Time, Avg Service Time, Throughput, Compute, Overhead).
5.3 Presentazione dei Risultati
Includere le tabelle complete dei dati.
Grafici Chiave: Throughput vs. N; Latenza vs. Periodo; Breakdown del tempo (Compute vs. Overhead) per i diversi kernel.
5.4 Discussione e Analisi dei Risultati
Validazione della Pipeline: Usare i dati (Latenza >> Periodo) per dimostrare che la sovrapposizione funziona.
Analisi del Collo di Bottiglia: Dimostrare come il sistema sia limitato dall'overhead (memory/call-bound) con i kernel leggeri, e come diventi limitato dal calcolo (compute-bound) con heavy_compute_kernel.
L'Overhead di Accodamento: Spiegare che l'alto Avg Overhead Time nei casi compute-bound è un sintomo di successo (tempo di attesa in coda) che prova la saturazione del collo di bottiglia.
Confronti Piattaforma:
CPU: FastFlow vs. OpenMP (perché OpenMP è risultato più veloce?).
GPU: OpenCL vs. Metal (quantificare il costo dell'astrazione vs. l'API nativa).
FPGA: Analisi delle performance HLS (perché è più lento? Overhead VM/PCIe, design non ottimizzato per la forza bruta).
   Cambio funzioni da enqueueBuffer in from explicit copy to map and migrate in FPGA Accelerator e GPU Accelerator
   Ho proivato a fare from explicit copy to map and migrate (vedi branch apposita) e si aveva effettivamente un incremento minimo nelle prestazioni ma si complicava ulteriormente il codice
   Es. con FPGA 
      ------------------------------------------------------------------
      PERFORMANCE METRICS on FPGA
         (N=7449999, Tasks=100, Kernel=krnl_heavy_compute)
      ------------------------------------------------------------------
      Avg Service Time: 2494.37 ms/task
         (Tempo medio tra il completamento di due task consecutivi)

      Avg In_Node Time: 132413 ms/task
         (Tempo medio per un task dall'ingresso all'uscita del nodo)

      Avg Pure Compute Time: 2545.6 ms/task
         (Tempo medio di un singolo calcolo sull'acceleratore, senza overhead)

      Avg Overhead Time: 129867 ms/task
         (Costo medio di gestione: trasferimento dati, uso delle code, etc.)

      Throughput: 0.392308 tasks/sec
         (Task totali processati al secondo)

      Total Time Elapsed: 254.902 s
      ------------------------------------------------------------------

Capitolo 6: Conclusioni
6.1 Sintesi del Lavoro e Raggiungimento degli Obiettivi
6.2 Limitazioni e Problemi Riscontrati
   (es. limite N su FPGA, 
   nome kernel uguale a nome kernel file, 
   incompatibilità di versione Vitis, 
   gestione errori con exit(), 
   design specializzato per kernel 2-in/1-out
   FF non funziona bene con 3 nodi).
6.4 Considerazioni Personali
(Cosa ha imparato dal progetto, corsi universitari più utili, difficoltà e soddisfazioni).

Bibliografia
   lista bullet points

Codice

Ringraziamenti