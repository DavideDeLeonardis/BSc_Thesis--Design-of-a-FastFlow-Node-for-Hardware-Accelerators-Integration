!! Migliora diagramma delle classi
!! Aggiungi citazioni
!! Aggiungi captions a immagini

Font: Cambria
Codice: Courier New
Capitoli: 18pt
Sezioni: 16pt
Sottosezione: 14pt
Testo: 12pt
4 righe fra sottosezioni diverse

TITOLO:
   Progettazione di un nodo FastFlow per integrazione di acceleratori

Capitolo 1: Introduzione
1.1 Contesto: L'Era del Calcolo Eterogeneo
Il rallentamento della Legge di Moore e la necessità di performance.
L'ascesa del calcolo eterogeneo (CPU, GPU, FPGA) come soluzione.
1.2 Il Problema: Complessità di Integrazione
La potenza degli acceleratori (GPU per il parallelismo di massa, FPGA per la specializzazione) vs. la difficoltà di programmarli e integrarli in applicazioni C++ esistenti.
1.3 La Soluzione Proposta: FastFlow come Orchestratore
Introduzione a FastFlow come framework di alto livello per il parallelismo C++.
Spiegare come FastFlow possa "nascondere" la complessità dell'offloading hardware.
1.4 Obiettivi e Contributi della Tesi
Obiettivo: Si propone di realizzare un semplice prototipo per l'integrazione di kernel programmati su FPGA Alveo mediante Vitis (sostanzialmente kernel scritti in C/C++) nell'ambiente di programmazione FastFlow. Verranno utilizzati kernel esistenti.
L'obiettivo della tesi sarà quello di interfacciare i kernel mediante codice OpenCL sfruttando tutte le possibilità offerte da tale ambiente. L'integrazione in FastFlow dovrà permettere di utilizzare sia il classico parallelismo shared memory/thread sulla CPU multicore che l'offloading di computazioni pesanti sull'acceleratore FPGA.
Progettare, implementare e valutare un nodo FastFlow per l'integrazione efficiente di kernel per acceleratori hardware (FPGA, GPU OpenCL, GPU Metal) all'interno di una pipeline FastFlow.
Contributi: Il design di un'architettura software disaccoppiata; l'analisi comparativa delle performance (Latenza, Throughput, Overhead) tra diverse API e carichi di lavoro (memory-bound vs. compute-bound).

Capitolo 2: Fondamenti e Tecnologie Abilitanti
2.1 Architetture a Confronto: CPU, GPU, FPGA
Architettura CPU (Multi-core, cache, shared memory).
Architettura GPU (SIMT, migliaia di core, parallelismo dei dati).
Architettura FPGA (Logica riconfigurabile, parallelismo a pipeline, Sintesi ad Alto Livello (HLS) e ruolo di Vitis).
2.2 Framework di Parallelismo Software su CPU
FastFlow: Spiegare i pattern ff_Pipe (pipeline) e ff::ParallelFor (parallelismo dati).
OpenMP: Spiegare il modello a direttive (#pragma omp parallel for) come standard industriale.
2.3 API per l'Offloading Hardware
OpenCL: Spiegare il suo ruolo di standard multipiattaforma (Contesto, Coda, Kernel .cl, Eventi).
Apple Metal: Spiegare il suo ruolo di API nativa ad alte prestazioni per macOS (Device, Coda, Kernel .metal, Memoria Unificata).

Capitolo 3: Progettazione dell'Architettura Software - Progetto logico
(Il "come" e il "perché" del suo design)
3.1 Visione d'Insieme: Un Dispatcher Flessibile
Descrivere l'architettura del main come "dispatcher" che seleziona il percorso di esecuzione (CPU vs. Acceleratore) in base agli input.
3.2 L'Interfaccia IAccelerator: Il Contratto di Astrazione
Spiegare l'importanza di un'interfaccia pura per disaccoppiare la logica della pipeline dall'hardware specifico.
3.3 Architettura 1: Esecuzione Parallela su CPU - Desing pattern usato
Spiegare il design dei "Runner" (executeCpuFFTasks, executeCpuOMPTasks) e perché sono sequenziali a livello di task ma paralleli a livello di dati.
3.4 Architettura 2: La Pipeline di Offloading (ff_node_acc_t)
Il Problema del Design: Come gestire l'offloading asincrono e "stateful" (con stato) all'interno di un nodo FastFlow?
La Soluzione (Il Cuore della Tesi): L'incapsulamento di una pipeline manuale a 2 stadi (producer/consumer).
Giustificazione: Spiegare perché questa architettura è superiore a una ff_Pipe "piatta" (gestione centralizzata dello stato, del BufferManager e della sincronizzazione).
Sincronizzazione Efficiente: Spiegare la scelta della BlockingQueue per eliminare l'attesa attiva (yield) e consumare 0% di CPU.
3.5 Pattern di Composizione: Il BufferManager
Spiegare perché la logica del pool di buffer è stata estratta in una classe separata (Composizione) invece di usare una classe base complessa (Ereditarietà), per eliminare la duplicazione di codice tra Gpu e Fpga_Accelerator.

Capitolo 4: Dettagli Implementativi
(Il "codice" che realizza il design)
ff_node_acc_t
4.2 Implementazione degli Acceleratori Concreti
Gpu_OpenCL_Accelerator: Compilazione da sorgente (clCreateProgramWithSource).
Fpga_Accelerator: Caricamento di binari (clCreateProgramWithBinary).
Gpu_Metal_Accelerator: L'uso di Objective-C++ (.mm), la gestione della memoria unificata (memcpy), la compilazione da sorgente (newLibraryWithSource) e i "bridging cast" per nascondere i tipi (void*).
4.3 Implementazione della Portabilità (CMake)
Spiegare come CMakeLists.txt usa le direttive condizionali (if(APPLE)) per compilare e linkare file diversi (es. Fpga_Accelerator.cpp solo su Linux, Gpu_Metal_Accelerator.mm solo su macOS).

Capitolo 5: Analisi Sperimentale e Risultati
(Il capitolo più importante: i dati)
5.1 Setup Sperimentale (Hardware e Software)
Descrivere in dettaglio le macchine (Mac M2 Pro, Host linux con ubunti con cpu Intel e Scheda FPGA Alveo U50 - comando per vedere dati delle schede) e le versioni dei software (Vitis, FastFlow, Clang, GCC).
5.2 Metodologia di Benchmark e Metriche
Spiegare i test (N=10k, 1M, 7.4M; NUM_TASKS=100).
Definizione delle Metriche: Spiegare la legenda (Avg In-Node Time, Avg Service Time, Throughput, Compute, Overhead).
5.3 Presentazione dei Risultati
Includere le tabelle complete dei dati.
Grafici Chiave: Throughput vs. N; Latenza vs. Periodo; Breakdown del tempo (Compute vs. Overhead) per i diversi kernel.
5.4 Discussione e Analisi dei Risultati
Validazione della Pipeline: Usare i dati (Latenza >> Periodo) per dimostrare che la sovrapposizione funziona.
Analisi del Collo di Bottiglia: Dimostrare come il sistema sia limitato dall'overhead (memory/call-bound) con i kernel leggeri, e come diventi limitato dal calcolo (compute-bound) con heavy_compute_kernel.
L'Overhead di Accodamento: Spiegare che l'alto Avg Overhead Time nei casi compute-bound è un sintomo di successo (tempo di attesa in coda) che prova la saturazione del collo di bottiglia.
Confronti Piattaforma:
CPU: FastFlow vs. OpenMP (perché OpenMP è risultato più veloce?).
GPU: OpenCL vs. Metal (quantificare il costo dell'astrazione vs. l'API nativa).
FPGA: Analisi delle performance HLS (perché è più lento? Overhead VM/PCIe, design non ottimizzato per la forza bruta).

Capitolo 6: Conclusioni
6.1 Sintesi del Lavoro e Raggiungimento degli Obiettivi
6.2 Limitazioni e Problemi Riscontrati
(es. nome kernel uguale a nome kernel file, incompatibilità di versione Vitis, limiti di memoria FPGA su VM, gestione errori con exit(), design specializzato per kernel 2-in/1-out).
6.4 Considerazioni Personali
(Cosa ha imparato dal progetto, corsi universitari più utili, difficoltà e soddisfazioni).
6.5 Sviluppi futuri come cambio funzioni enqueueBuffer in from explicit copy to map and migrate in FPGA Accelerator
   Hai notato che abbiamo cambiato la strategia di trasferimento dei dati rispetto al tuo codice originale per l'FPGA.

                  La risposta breve è: abbiamo scelto clEnqueue[Write/Read]Buffer perché è più semplice, più pulito e universalmente compatibile, rendendo il codice della GPU e dell'FPGA quasi identico e più facile da gestire nella nostra pipeline asincrona.

                  Entrambi i metodi, alla fine, fanno la stessa cosa: spostare un blocco di dati dalla RAM della CPU alla memoria del device (HBM sull'FPGA o VRAM sulla GPU). La differenza sta nel come lo fanno.

                  Metodo 1: clEnqueueWrite/ReadBuffer (Il nostro metodo attuale)
                  Questo è il modello di trasferimento implicito (o diretto).

                  Come funziona: È una singola chiamata API. Tu dici al driver OpenCL: "Prendi i dati da questo puntatore della RAM (task->a) e copiali in quel buffer sul device (current_buffers.bufferA). Avvisami quando hai finito (restituendo un evento)".

                  Chi fa il lavoro: Il driver OpenCL si occupa di tutto. Dietro le quinte, molto probabilmente alloca un pezzo di memoria "pinnata" (bloccata in RAM), ci copia i tuoi dati, e poi ordina al controller DMA (Direct Memory Access) di spostare i dati da lì al device.

                  Pro:

                  Semplicità: Una sola chiamata.

                  Portabilità: Funziona perfettamente e in modo identico su qualsiasi hardware OpenCL (CPU, GPU, FPGA).

                  Pulizia: Si integra perfettamente nella nostra pipeline basata su eventi (CL_FALSE per renderlo asincrono e &task->event per ottenere l'evento di completamento).

                  Metodo 2: Map / Migrate / Unmap (Il tuo metodo originale sull'FPGA)
                  Questo è il modello di trasferimento esplicito (o zero-copy-ish). È il modello fortemente raccomandato da Xilinx per le sue schede Alveo (FPGA).

                  Come funziona: È un processo in più fasi che gestisci tu manualmente.

                  clEnqueueMapBuffer: Chiedi al driver OpenCL: "Dammi un puntatore a una zona di RAM sull'host che sia 'mappata' sul buffer del device".

                  memcpy: Sei tu (la tua CPU) che scrivi fisicamente i dati in quella zona di RAM mappata.

                  clEnqueueMigrateMemObjects: Dici al driver: "Ok, ho finito di scrivere. Ora sei autorizzato a iniziare il trasferimento DMA di quella zona di memoria verso la memoria fisica del device (HBM)".

                  clEnqueueUnmapMemObject: Rilasci il puntatore mappato.

                  Pro:

                  Performance (Teorica) su FPGA: Questo modello è progettato per evitare copie intermedie. Scrivendo direttamente in una memoria "host-accessible" che il DMA può leggere, si ottiene (in teoria) il percorso dati più veloce possibile sull'hardware Xilinx.

                  Contro:

                  Complessità: È molto più verboso. Richiede 3-4 chiamate API e la gestione manuale di memcpy.

                  Orchestrazione Asincrona Difficile: Gestire la catena di eventi (Map deve finire prima di memcpy, memcpy prima di Migrate, Migrate prima del Kernel, Kernel prima del Migrate di ritorno...) è molto più complicato che con una singola chiamata.

                  Specificità: È un modello molto specifico per architetture con memoria unificata o bus ad alte prestazioni come PCIe, tipico delle FPGA Xilinx.

                  Perché Abbiamo Scelto il Metodo 1?
                  Per l'obiettivo della tua tesi, ovvero dimostrare l'efficacia di una pipeline software asincrona a livello di sistema operativo/FastFlow, i vantaggi del Metodo 1 erano molto superiori:

                  Priorità alla Chiarezza dell'Architettura: La nostra priorità era costruire una pipeline a 3 stadi pulita. WriteBuffer ci ha permesso di avere uno stadio send_data_async semplice e comprensibile. Gestire la complessità del Map/Migrate avrebbe "sporcato" la logica della pipeline.

                  Astrazione e Portabilità: Usando WriteBuffer, i nostri GpuAccelerator e FpgaAccelerator sono diventati quasi identici. Questo è un risultato potente: dimostra che la nostra architettura (ff_node_acc_t) è così ben progettata da poter orchestrare hardware completamente diversi usando la stessa interfaccia.

                  "Good Enough" Performance: Anche se Map/Migrate potrebbe essere marginalmente più veloce sull'FPGA, WriteBuffer è comunque estremamente performante. Il collo di bottiglia che stiamo risolvendo (il tempo di attesa della CPU) è molto più grande della piccola differenza tra questi due metodi di copia.

                  In sintesi: abbiamo preferito la semplicità, la portabilità e la pulizia dell'architettura (WriteBuffer) rispetto a una micro-ottimizzazione specifica per l'hardware (Map/Migrate) che avrebbe reso il codice molto più complesso.

Bibliografia
   lista bullet points

Codice

Ringraziamenti